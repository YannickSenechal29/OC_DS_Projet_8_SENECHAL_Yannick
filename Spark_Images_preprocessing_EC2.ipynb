{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8670ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, functions as F\n",
    "from pyspark.sql.types import BinaryType, ArrayType, IntegerType, StringType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import PCA\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af3bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os environnement definition for java and hadoop with aws s3\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f1ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define id and key for credential in S3\n",
    "access_key_ID = os.environ['AccessKeyId']\n",
    "access_secret_key = os.environ['AccessKeySecret'] \n",
    "#SparkSession set up\n",
    "spark = SparkSession\\\n",
    "         .builder.master('local[*]')\\\n",
    "         .appName('images_preprocessing_aws')\\\n",
    "         .config('spark.driver.memory', '2g')\\\n",
    "         .config('spark.hadoop.fs.s3a.access.key', access_key_ID)\\\n",
    "         .config('spark.hadoop.fs.s3a.secret.key', access_secret_key)\\\n",
    "         .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\\\n",
    "         .getOrCreate()\n",
    "# Trying to optimise memory usage\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")        \n",
    "#Launch & access SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f298393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 path\n",
    "path_s3 = 's3a://oc-projet8-yannick/fruits_images_samples_t2micro/'\n",
    "path_s3_root = 's3a://oc-projet8-yannick/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4475fd",
   "metadata": {},
   "source": [
    "### **Images loading in dataframe spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d0aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               image|\n",
      "+--------------------+\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "|[s3a://oc-projet8...|\n",
      "+--------------------+\n",
      "\n",
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_images = spark.read.format('image').load(f'{path_s3}/*')\n",
    "df_images.show()\n",
    "df_images.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa200e",
   "metadata": {},
   "source": [
    "### **Let's get the dataframe in the format that we want, add path and label column and drop 'image' column'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa6d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+---------+--------------+--------------------+\n",
      "|       image_path_s3|height|width|nChannels|         label|                data|\n",
      "+--------------------+------+-----+---------+--------------+--------------------+\n",
      "|oc-projet8-yannic...|   100|  100|        3|   Apple_Red_1|[FF FD FF FF FD F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|   Apple_Red_1|[FF FF FF FF FF F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Apple_Braeburn|[FF FF FE FF FF F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Cherry_Rainier|[FF FF F8 FF FF F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Apple_Braeburn|[FF FF FE FF FF F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Cherry_Rainier|[FF FE FF FF FE F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|       Avocado|[FF FF FF FF FF F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|       Avocado|[FF FF FF FF FF F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|  Cactus_fruit|[FF FF FF FF FF F...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|  Cactus_fruit|[FF FF FF FF FF F...|\n",
      "+--------------------+------+-----+---------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting Image path with origin column\n",
    "# getting some other information on the same level of column image\n",
    "# getting image labels with the name of the folder, be carreful of [2] should be changed in function of the path\n",
    "# getting data bytes of images\n",
    "df_images = df_images.withColumn('image_path_s3', F.split(F.col('image.origin'), 's3a://')[1])\\\n",
    ".withColumn('height', F.col('image.height'))\\\n",
    ".withColumn('width', F.col('image.width')).withColumn('nChannels', F.col('image.nChannels'))\\\n",
    ".withColumn('label',(F.split((F.split(F.col('image.origin'), 's3a://')[1]), '/'))[2])\\\n",
    ".withColumn('data', F.col('image.data'))\n",
    "# droping image column\n",
    "df_images = df_images.drop(F.col('image'))\n",
    "# display the new dataframe with our images data\n",
    "df_images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3847d",
   "metadata": {},
   "source": [
    "### Define and perform preprocessing function with UDF (for distributed calcul) and return new columns into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c4fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "def preprocess_images(data_bytes):\n",
    "    '''Function to preprocess distribute image data with openCV\n",
    "    we enter the image data from DataFrame Spark and return an array of integer to work with opencv on it\n",
    "    The images are threated with blurrig and equalization function and resize in 32X32'''\n",
    "    # getting data from image and pass it to img array shape\n",
    "    img_data_bgr = np.array(data_bytes).reshape(100,100,3)\n",
    "    # getting gray images\n",
    "    img_data_gray = cv.cvtColor(img_data_bgr, cv.COLOR_BGR2GRAY)\n",
    "    # Blurring\n",
    "    img_data_gray = cv.GaussianBlur(img_data_gray, (3,3), 0)\n",
    "    # Equalization\n",
    "    img_data_gray = cv.equalizeHist(img_data_gray)\n",
    "    # Resize\n",
    "    img_data_gray = cv.resize(img_data_gray, (32,32))\n",
    "    # flatten the result\n",
    "    img_data_gray = img_data_gray.flatten()\n",
    "    # forcing python list for the result instead of numpy array for spark to understand\n",
    "    img_data_gray = img_data_gray.tolist()\n",
    "    return img_data_gray\n",
    "\n",
    "#UDF definition with returning array of integer for spark\n",
    "udf_img_preprocess = F.udf(preprocess_images, ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72575156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+---------+--------------+--------------------+--------------------+\n",
      "|       image_path_s3|height|width|nChannels|         label|                data|     preprocess_data|\n",
      "+--------------------+------+-----+---------+--------------+--------------------+--------------------+\n",
      "|oc-projet8-yannic...|   100|  100|        3|   Apple_Red_1|[FF FD FF FF FD F...|[255, 255, 228, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|   Apple_Red_1|[FF FF FF FF FF F...|[255, 255, 255, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Apple_Braeburn|[FF FF FE FF FF F...|[255, 255, 247, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Cherry_Rainier|[FF FF F8 FF FF F...|[225, 225, 255, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Apple_Braeburn|[FF FF FE FF FF F...|[255, 255, 255, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|Cherry_Rainier|[FF FE FF FF FE F...|[228, 255, 255, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|       Avocado|[FF FF FF FF FF F...|[255, 255, 255, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|       Avocado|[FF FF FF FF FF F...|[255, 255, 255, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|  Cactus_fruit|[FF FF FF FF FF F...|[255, 255, 255, 2...|\n",
      "|oc-projet8-yannic...|   100|  100|        3|  Cactus_fruit|[FF FF FF FF FF F...|[255, 255, 255, 2...|\n",
      "+--------------------+------+-----+---------+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create column data_preprocess with flatten data\n",
    "df_images = df_images.withColumn('preprocess_data',udf_img_preprocess(df_images.data))\n",
    "df_images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751734c",
   "metadata": {},
   "source": [
    "### **Preprocessing results formating before performing PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40125a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our preprocess data to vector for PCA model via udf fonction to transform array into vector\n",
    "to_vector = F.udf(lambda x: Vectors.dense(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4685760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|     preprocess_data|preprocess_data_vector|\n",
      "+--------------------+----------------------+\n",
      "|[255, 255, 228, 2...|  [255.0,255.0,228....|\n",
      "|[255, 255, 255, 2...|  [255.0,255.0,255....|\n",
      "|[255, 255, 247, 2...|  [255.0,255.0,247....|\n",
      "|[225, 225, 255, 2...|  [225.0,225.0,255....|\n",
      "|[255, 255, 255, 2...|  [255.0,255.0,255....|\n",
      "|[228, 255, 255, 2...|  [228.0,255.0,255....|\n",
      "|[255, 255, 255, 2...|  [255.0,255.0,255....|\n",
      "|[255, 255, 255, 2...|  [255.0,255.0,255....|\n",
      "|[255, 255, 255, 2...|  [255.0,255.0,255....|\n",
      "|[255, 255, 255, 2...|  [255.0,255.0,255....|\n",
      "+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating vector column\n",
    "df_images = df_images.withColumn('preprocess_data_vector', to_vector(df_images.preprocess_data))\n",
    "df_images[['preprocess_data', 'preprocess_data_vector']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc7ab9",
   "metadata": {},
   "source": [
    "### **Performing PCA with our vector data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ee32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_number = 10\n",
    "pca = PCA(k=k_number, inputCol=\"preprocess_data_vector\")\n",
    "pca.setOutputCol(\"pca_data_vector\")\n",
    "model = pca.fit(df_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4afcc525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+\n",
      "|preprocess_data_vector|     pca_data_vector|\n",
      "+----------------------+--------------------+\n",
      "|  [255.0,255.0,228....|[-661.95527199896...|\n",
      "|  [255.0,255.0,255....|[-682.96144962879...|\n",
      "|  [255.0,255.0,247....|[-289.36356177605...|\n",
      "|  [225.0,225.0,255....|[-1684.4265943424...|\n",
      "|  [255.0,255.0,255....|[-276.34928556947...|\n",
      "|  [228.0,255.0,255....|[-1654.2687498265...|\n",
      "|  [255.0,255.0,255....|[-2686.0682303350...|\n",
      "|  [255.0,255.0,255....|[-2710.1059529450...|\n",
      "|  [255.0,255.0,255....|[-3746.6211286444...|\n",
      "|  [255.0,255.0,255....|[-3731.0603346980...|\n",
      "+----------------------+--------------------+\n",
      "\n",
      "Explained_Variance_Ratio: 0.9999999999999826\n"
     ]
    }
   ],
   "source": [
    "df_PCA = model.transform(df_images)\n",
    "df_PCA[['preprocess_data_vector', 'pca_data_vector']].show()\n",
    "print('Explained_Variance_Ratio:', model.explainedVariance.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b84ab",
   "metadata": {},
   "source": [
    "### **Saving our output file on S3 (by s3a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302a58d",
   "metadata": {},
   "source": [
    "#### **Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d81f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA.select('image_path_s3', 'label', 'pca_data_vector').write.mode('overwrite')\\\n",
    ".parquet(f'{path_s3_root}output/PCA_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389d5cf",
   "metadata": {},
   "source": [
    "#### **CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4df69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert denseVector into string for saving in CSV, spark does'nt accept array vector format to save into csv\n",
    "vector_to_string = F.udf(lambda x: str(x), StringType())\n",
    "df_PCA = df_PCA.withColumn('pca_data_vector_str', vector_to_string(df_PCA.pca_data_vector))\n",
    "# saving to CSV with column image_path, label and pca_vector_str\n",
    "df_PCA.select('image_path_s3', 'label', 'pca_data_vector_str').write.option(\"header\", True).mode('overwrite')\\\n",
    ".csv(f'{path_s3_root}output/PCA_results_CSV')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
